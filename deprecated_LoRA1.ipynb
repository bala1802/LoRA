{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.nn.utils.parametrize as parametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "from mnist_model import MNIST_MODEL\n",
    "import config\n",
    "import train\n",
    "import test\n",
    "import model_utils\n",
    "import lora_parameterization\n",
    "from fine_tune import fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_loader.load_mnist_dataset(train=True)\n",
    "train_data_loader = data_loader.data_loader(dataset=train_dataset)\n",
    "\n",
    "test_dataset = data_loader.load_mnist_dataset(train=False)\n",
    "test_data_loader = data_loader.data_loader(dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1000]         785,000\n",
      "              ReLU-2                 [-1, 1000]               0\n",
      "            Linear-3                 [-1, 2000]       2,002,000\n",
      "              ReLU-4                 [-1, 2000]               0\n",
      "            Linear-5                   [-1, 10]          20,010\n",
      "              ReLU-6                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 2,807,010\n",
      "Trainable params: 2,807,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 10.71\n",
      "Estimated Total Size (MB): 10.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_MODEL()\n",
    "summary(model, input_size=(1,28,28))\n",
    "model = model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:50<00:00, 118.15it/s, loss=2.07]\n"
     ]
    }
   ],
   "source": [
    "train.train(model=model, data_loader=train_data_loader, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Trained model's original weights are preserved, this is represented by `W` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = model_utils.extract_weights(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:10<00:00, 97.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21\n",
      "Wrong Counts for the digit 0 : 1\n",
      "Wrong Counts for the digit 1 : 13\n",
      "Wrong Counts for the digit 2 : 1032\n",
      "Wrong Counts for the digit 3 : 1010\n",
      "Wrong Counts for the digit 4 : 982\n",
      "Wrong Counts for the digit 5 : 892\n",
      "Wrong Counts for the digit 6 : 958\n",
      "Wrong Counts for the digit 7 : 1028\n",
      "Wrong Counts for the digit 8 : 974\n",
      "Wrong Counts for the digit 9 : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test.test(model=model, data_loader=test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the parameters for each layer in the original network, before applying LoRA matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W (Matrix): torch.Size([1000, 784]) + Bias : torch.Size([1000])\n",
      "Layer 2: W (Matrix): torch.Size([2000, 1000]) + Bias : torch.Size([2000])\n",
      "Layer 3: W (Matrix): torch.Size([10, 2000]) + Bias : torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "original_parameters_count = model_utils.count_total_parameters(\n",
    "    model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "print(f'Total number of parameters: {original_parameters_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the LoRA parameterization to the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=2000, out_features=10, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRA_Parameterization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_1, \"weight\", lora_parameterization.layer_parameterization(layer=model.linear_layer_1)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_2, \"weight\", lora_parameterization.layer_parameterization(layer=model.linear_layer_2)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_3, \"weight\", lora_parameterization.layer_parameterization(layer=model.linear_layer_3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's visualize the Number of Parameters added by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer - 1: W (weight matrix) : torch.Size([1000, 784]) + B (bias): torch.Size([1000]) + LoRA A Matrix: torch.Size([1, 784])+ LoRA B Matrix: torch.Size([1000, 1])\n",
      "Layer - 2: W (weight matrix) : torch.Size([2000, 1000]) + B (bias): torch.Size([2000]) + LoRA A Matrix: torch.Size([1, 1000])+ LoRA B Matrix: torch.Size([2000, 1])\n",
      "Layer - 3: W (weight matrix) : torch.Size([10, 2000]) + B (bias): torch.Size([10]) + LoRA A Matrix: torch.Size([1, 2000])+ LoRA B Matrix: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "lora_parameters_count = 0\n",
    "non_lora_parameters_count = 0\n",
    "\n",
    "for index, layer in enumerate([model.linear_layer_1, model.linear_layer_2, model.linear_layer_3]):\n",
    "    lora_parameters_count += layer.parametrizations[\"weight\"][0].LoRA_Matrix_A.nelement() + layer.parametrizations[\"weight\"][0].LoRA_Matrix_B.nelement()\n",
    "    non_lora_parameters_count += layer.weight.nelement() + layer.bias.nelement()\n",
    "\n",
    "    print(f'Layer - {index+1}: W (weight matrix) : {layer.weight.shape} + B (bias): {layer.bias.shape} + LoRA A Matrix: {layer.parametrizations[\"weight\"][0].LoRA_Matrix_A.shape}+ LoRA B Matrix: {layer.parametrizations[\"weight\"][0].LoRA_Matrix_B.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate whether the parameter counts were distrubed to the original trained model or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Parameters count : 2807010\n",
      "Total Parameters after registering LoRA: Original Parameters Count + LoRA Parameters Count = 2813804\n",
      "Parameters introduced by LoRA : 6794\n",
      "Parameters incremented by LoRA : 0.242%\n"
     ]
    }
   ],
   "source": [
    "assert non_lora_parameters_count == original_parameters_count\n",
    "print(f'Original Parameters count : {original_parameters_count}')\n",
    "print(f'Total Parameters after registering LoRA: Original Parameters Count + LoRA Parameters Count = {original_parameters_count + lora_parameters_count}')\n",
    "print(f'Parameters introduced by LoRA : {lora_parameters_count}')\n",
    "\n",
    "parameters_incremented_by_lora = (lora_parameters_count / non_lora_parameters_count) * 100\n",
    "print(f'Parameters incremented by LoRA : {parameters_incremented_by_lora:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, lets work on improving the accuracy for the digit `2`. To do that, lets freeze all the Parameters in the original model and finetune only the parameters introduced by LoRA. The finetuning is done only on the digit `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Non LoRA layer parameter linear_layer_1.bias\n",
      "Freezing Non LoRA layer parameter linear_layer_1.parametrizations.weight.original\n",
      "Unfreeze Layer found :  linear_layer_1.parametrizations.weight.0.LoRA_Matrix_A\n",
      "Unfreeze Layer found :  linear_layer_1.parametrizations.weight.0.LoRA_Matrix_B\n",
      "Freezing Non LoRA layer parameter linear_layer_2.bias\n",
      "Freezing Non LoRA layer parameter linear_layer_2.parametrizations.weight.original\n",
      "Unfreeze Layer found :  linear_layer_2.parametrizations.weight.0.LoRA_Matrix_A\n",
      "Unfreeze Layer found :  linear_layer_2.parametrizations.weight.0.LoRA_Matrix_B\n",
      "Freezing Non LoRA layer parameter linear_layer_3.bias\n",
      "Freezing Non LoRA layer parameter linear_layer_3.parametrizations.weight.original\n",
      "Unfreeze Layer found :  linear_layer_3.parametrizations.weight.0.LoRA_Matrix_A\n",
      "Unfreeze Layer found :  linear_layer_3.parametrizations.weight.0.LoRA_Matrix_B\n"
     ]
    }
   ],
   "source": [
    "model_utils.freeze_model_parameters(model=model, unfreeze_layer=\"LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_loader.load_mnist_dataset(train=True)\n",
    "indices_digit_2 = train_dataset.targets == 2\n",
    "train_dataset.data = train_dataset.data[indices_digit_2]\n",
    "train_dataset.targets = train_dataset.targets[indices_digit_2]\n",
    "train_data_loader = data_loader.data_loader(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning the model with the updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:01<00:00, 62.66it/s, loss=2.32] \n"
     ]
    }
   ],
   "source": [
    "fine_tune(epochs=1, model=model, total_iterations_limit=100, train_loader=train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(model.linear_layer_1.parametrizations.weight.original == weights_dict['linear_layer_1.weight'])\n",
    "assert torch.all(model.linear_layer_2.parametrizations.weight.original == weights_dict['linear_layer_2.weight'])\n",
    "assert torch.all(model.linear_layer_3.parametrizations.weight.original == weights_dict['linear_layer_3.weight'])\n",
    "\n",
    "lora_parameterization.enable_or_disable_lora(enabled=True, model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "assert torch.equal(model.linear_layer_1.weight, model.linear_layer_1.parametrizations.weight.original + \n",
    "                   (model.linear_layer_1.parametrizations.weight[0].LoRA_Matrix_B \n",
    "                    @ model.linear_layer_1.parametrizations.weight[0].LoRA_Matrix_A) \n",
    "                   * model.linear_layer_1.parametrizations.weight[0].scale)\n",
    "\n",
    "lora_parameterization.enable_or_disable_lora(enabled=False, model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "assert torch.equal(model.linear_layer_1.weight, weights_dict['linear_layer_1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNIST_MODEL(\n",
       "  (linear_layer_1): ParametrizedLinear(\n",
       "    in_features=784, out_features=1000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRA_Parameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_layer_2): ParametrizedLinear(\n",
       "    in_features=1000, out_features=2000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRA_Parameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_layer_3): ParametrizedLinear(\n",
       "    in_features=2000, out_features=10, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRA_Parameterization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:11<00:00, 86.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21\n",
      "Wrong Counts for the digit 0 : 1\n",
      "Wrong Counts for the digit 1 : 13\n",
      "Wrong Counts for the digit 2 : 1032\n",
      "Wrong Counts for the digit 3 : 1010\n",
      "Wrong Counts for the digit 4 : 982\n",
      "Wrong Counts for the digit 5 : 892\n",
      "Wrong Counts for the digit 6 : 958\n",
      "Wrong Counts for the digit 7 : 1028\n",
      "Wrong Counts for the digit 8 : 974\n",
      "Wrong Counts for the digit 9 : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_parameterization.enable_or_disable_lora(enabled=True, \n",
    "                                             model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "test.test(model=model, data_loader=test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:10<00:00, 95.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21\n",
      "Wrong Counts for the digit 0 : 1\n",
      "Wrong Counts for the digit 1 : 13\n",
      "Wrong Counts for the digit 2 : 1032\n",
      "Wrong Counts for the digit 3 : 1010\n",
      "Wrong Counts for the digit 4 : 982\n",
      "Wrong Counts for the digit 5 : 892\n",
      "Wrong Counts for the digit 6 : 958\n",
      "Wrong Counts for the digit 7 : 1028\n",
      "Wrong Counts for the digit 8 : 974\n",
      "Wrong Counts for the digit 9 : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_parameterization.enable_or_disable_lora(enabled=False, \n",
    "                                             model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "test.test(model=model, data_loader=test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
