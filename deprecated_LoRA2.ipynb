{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "from mnist_model import MNIST_MODEL\n",
    "import config\n",
    "import train\n",
    "import test\n",
    "import model_utils\n",
    "from lora_model import LoRA_Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_loader.load_mnist_dataset(train=True)\n",
    "train_data_loader = data_loader.data_loader(dataset=train_dataset)\n",
    "\n",
    "test_dataset = data_loader.load_mnist_dataset(train=False)\n",
    "test_data_loader = data_loader.data_loader(dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1000]         785,000\n",
      "              ReLU-2                 [-1, 1000]               0\n",
      "            Linear-3                 [-1, 2000]       2,002,000\n",
      "              ReLU-4                 [-1, 2000]               0\n",
      "            Linear-5                   [-1, 10]          20,010\n",
      "              ReLU-6                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 2,807,010\n",
      "Trainable params: 2,807,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 10.71\n",
      "Estimated Total Size (MB): 10.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_MODEL()\n",
    "summary(model, input_size=(1,28,28))\n",
    "model = model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:48<00:00, 124.20it/s, loss=2.07]\n"
     ]
    }
   ],
   "source": [
    "train.train(model=model, data_loader=train_data_loader, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clone Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = model_utils.extract_weights(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:08<00:00, 112.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21\n",
      "Wrong Counts for the digit 0 : 1\n",
      "Wrong Counts for the digit 1 : 13\n",
      "Wrong Counts for the digit 2 : 1032\n",
      "Wrong Counts for the digit 3 : 1010\n",
      "Wrong Counts for the digit 4 : 982\n",
      "Wrong Counts for the digit 5 : 892\n",
      "Wrong Counts for the digit 6 : 958\n",
      "Wrong Counts for the digit 7 : 1028\n",
      "Wrong Counts for the digit 8 : 974\n",
      "Wrong Counts for the digit 9 : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test.test(model=model, data_loader=test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Number of parameters present in the model, before loading the LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W (Matrix): torch.Size([1000, 784]) + Bias : torch.Size([1000])\n",
      "Layer 2: W (Matrix): torch.Size([2000, 1000]) + Bias : torch.Size([2000])\n",
      "Layer 3: W (Matrix): torch.Size([10, 2000]) + Bias : torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "original_parameters_count = model_utils.count_total_parameters(model_layers=[model.linear_layer_1, model.linear_layer_2, model.linear_layer_3])\n",
    "print(f'Total number of parameters: {original_parameters_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Layer Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_parameterization(layer, device):\n",
    "    d, k = layer.weight.shape\n",
    "    return LoRA_Parameterization(d=d, k=k, rank=config.RANK, alpha=config.ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Registering the Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=2000, out_features=10, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRA_Parameterization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_1, \"weight\", linear_layer_parameterization(model.linear_layer_1, device=config.DEVICE)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_2, \"weight\", linear_layer_parameterization(model.linear_layer_2, device=config.DEVICE)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear_layer_3, \"weight\", linear_layer_parameterization(model.linear_layer_3, device=config.DEVICE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_or_disable_lora(enabled=True):\n",
    "    for layer in [model.linear_layer_1, model.linear_layer_2, model.linear_layer_3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the Non-LoRA parameter linear_layer_1.bias\n",
      "Freezing the Non-LoRA parameter linear_layer_1.parametrizations.weight.original\n",
      "Freezing the Non-LoRA parameter linear_layer_2.bias\n",
      "Freezing the Non-LoRA parameter linear_layer_2.parametrizations.weight.original\n",
      "Freezing the Non-LoRA parameter linear_layer_3.bias\n",
      "Freezing the Non-LoRA parameter linear_layer_3.parametrizations.weight.original\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'LoRA' not in name:\n",
    "        print(f'Freezing the Non-LoRA parameter {name}')\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Domain Specific dataset for Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_2_dataset = data_loader.load_mnist_dataset(train=True)\n",
    "dataset_with_index_2 = version_2_dataset.targets == 9\n",
    "version_2_dataset.data = version_2_dataset.data[dataset_with_index_2]\n",
    "version_2_dataset.targets = version_2_dataset.targets[dataset_with_index_2]\n",
    "version_2_train_loader = data_loader.data_loader(dataset=version_2_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 100.53it/s, loss=2.31]\n"
     ]
    }
   ],
   "source": [
    "def fine_tune(train_loader, model, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(config.DEVICE)\n",
    "            y = y.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "fine_tune(train_loader=version_2_train_loader, model=model, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the domain specific fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:11<00:00, 86.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21\n",
      "Wrong Counts for the digit 0 : 1\n",
      "Wrong Counts for the digit 1 : 13\n",
      "Wrong Counts for the digit 2 : 1032\n",
      "Wrong Counts for the digit 3 : 1010\n",
      "Wrong Counts for the digit 4 : 982\n",
      "Wrong Counts for the digit 5 : 892\n",
      "Wrong Counts for the digit 6 : 958\n",
      "Wrong Counts for the digit 7 : 1028\n",
      "Wrong Counts for the digit 8 : 974\n",
      "Wrong Counts for the digit 9 : 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_or_disable_lora(enabled=True)\n",
    "test.test(data_loader=test_data_loader, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
